{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPo+KEP/zZPqUlJJafztAF/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPGREcajrX6Z","executionInfo":{"status":"ok","timestamp":1747469169906,"user_tz":-180,"elapsed":16869,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}},"outputId":"f7da0cc7-5bd7-4427-9e40-d2c2c09cc79b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","folder_path = '/content/drive/MyDrive/telecom/5sem'\n","video = os.path.join(folder_path, \"person15_running_d1_uncomp.avi\")"],"metadata":{"id":"TmlGUcDXrjTa","executionInfo":{"status":"ok","timestamp":1747469169907,"user_tz":-180,"elapsed":13,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["video = []\n","for f in os.listdir(path = folder_path):\n","    if f[:6] == 'person':\n","        video.append(os.path.join(folder_path, f))"],"metadata":{"id":"w2wUgQ-osSLm","executionInfo":{"status":"ok","timestamp":1747469170561,"user_tz":-180,"elapsed":656,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"dD9sfNwvrF4C","executionInfo":{"status":"ok","timestamp":1747469170583,"user_tz":-180,"elapsed":20,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class ConvLSTMCell(nn.Module):\n","    def __init__(self, input_channels, hidden_channels, kernel_size=3):\n","        super().__init__()\n","        padding = kernel_size // 2\n","        self.input_channels = input_channels\n","        self.hidden_channels = hidden_channels\n","\n","        self.conv = nn.Conv2d(\n","            input_channels + hidden_channels,\n","            4 * hidden_channels,\n","            kernel_size,\n","            padding=padding\n","        )\n","\n","    def forward(self, x, h_prev, c_prev):\n","        combined = torch.cat([x, h_prev], dim=1)\n","        conv_output = self.conv(combined)\n","\n","        cc_i, cc_f, cc_o, cc_g = torch.chunk(conv_output, 4, dim=1)\n","        i = torch.sigmoid(cc_i)\n","        f = torch.sigmoid(cc_f)\n","        o = torch.sigmoid(cc_o)\n","        g = torch.tanh(cc_g)\n","\n","        c = f * c_prev + i * g\n","        h = o * torch.tanh(c)\n","\n","        return h, c"]},{"cell_type":"code","source":["class ConvLSTM(nn.Module):\n","    def __init__(self, input_channels=1, hidden_channels=64, kernel_size=3, pred_len=5):\n","        super().__init__()\n","        self.cell = ConvLSTMCell(input_channels, hidden_channels, kernel_size)\n","        self.conv_out = nn.Conv2d(hidden_channels, input_channels, kernel_size=1)\n","        self.pred_len = pred_len\n","\n","    def forward(self, x_seq):\n","        B, T, C, H, W = x_seq.shape\n","        h = torch.zeros(B, self.cell.hidden_channels, H, W, device=x_seq.device)\n","        c = torch.zeros_like(h)\n","\n","        for t in range(T):\n","            h, c = self.cell(x_seq[:, t], h, c)\n","\n","        outputs = []\n","        x_in = x_seq[:, -1]\n","        for _ in range(self.pred_len):\n","            h, c = self.cell(x_in, h, c)\n","            x_in = self.conv_out(h)\n","            outputs.append(x_in)\n","\n","        return torch.stack(outputs, dim=1)\n"],"metadata":{"id":"MT7652rvrQO6","executionInfo":{"status":"ok","timestamp":1747469170593,"user_tz":-180,"elapsed":11,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import cv2\n","\n","def check_info_about_video(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","\n","    if not cap.isOpened():\n","        print(\"–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ.\")\n","    else:\n","        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","\n","        ret, frame = cap.read()\n","        if ret:\n","            channels = frame.shape[2] if len(frame.shape) == 3 else 1\n","\n","            print(f'–ö–∞–¥—Ä–æ–≤: {frame_count}')\n","            print(f'–†–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞: {width}x{height}')\n","            print(f'–ß–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤: {channels}')\n","            print(f'FPS: {fps}')\n","        else:\n","            print(\"–ù–µ —É–¥–∞–ª–æ—Å—å —Å—á–∏—Ç–∞—Ç—å –ø–µ—Ä–≤—ã–π –∫–∞–¥—Ä.\")\n","\n","    cap.release()\n","for _ in video:\n","    check_info_about_video(_)\n","    print('\\n\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FldLAAGsJS_","executionInfo":{"status":"ok","timestamp":1747469175050,"user_tz":-180,"elapsed":4454,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}},"outputId":"948dc7b8-b1aa-4d52-adf4-7c89ffbc45de"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["–ö–∞–¥—Ä–æ–≤: 432\n","–†–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞: 160x120\n","–ß–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤: 3\n","FPS: 25.0\n","\n","\n","\n","–ö–∞–¥—Ä–æ–≤: 160\n","–†–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞: 160x120\n","–ß–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤: 3\n","FPS: 25.0\n","\n","\n","\n","–ö–∞–¥—Ä–æ–≤: 206\n","–†–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞: 160x120\n","–ß–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤: 3\n","FPS: 25.0\n","\n","\n","\n","–ö–∞–¥—Ä–æ–≤: 581\n","–†–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞: 160x120\n","–ß–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤: 3\n","FPS: 25.0\n","\n","\n","\n","–ö–∞–¥—Ä–æ–≤: 312\n","–†–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞: 160x120\n","–ß–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤: 3\n","FPS: 25.0\n","\n","\n","\n","–ö–∞–¥—Ä–æ–≤: 408\n","–†–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞: 160x120\n","–ß–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤: 3\n","FPS: 25.0\n","\n","\n","\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","def load_video_tensor(path):\n","    cap = cv2.VideoCapture(path)\n","    frames = []\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, (160, 120))\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        frame = frame.astype(np.float32) / 255.0\n","        frames.append(frame)\n","\n","    cap.release()\n","\n","\n","    video_np = np.stack(frames)\n","    video_np = np.transpose(video_np, (0, 3, 1, 2))\n","    video_tensor = torch.from_numpy(video_np)\n","\n","    return video_tensor\n"],"metadata":{"id":"C5LqkdEUtcaG","executionInfo":{"status":"ok","timestamp":1747469175070,"user_tz":-180,"elapsed":8,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["video_tensor = load_video_tensor(video[0])\n","print(video_tensor.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y-956S1MtjMs","executionInfo":{"status":"ok","timestamp":1747469175403,"user_tz":-180,"elapsed":331,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}},"outputId":"8a9f1093-114a-4f97-dcf4-9e29da277b76"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([432, 3, 120, 160])\n"]}]},{"cell_type":"code","source":["import cv2\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset\n","\n","class VideoFrameDataset(Dataset):\n","    def __init__(self, video_paths, seq_len=10, pred_len=5):\n","        self.samples = []\n","        self.seq_len = seq_len\n","        self.pred_len = pred_len\n","\n","        for path in video_paths:\n","            video = self.load_video_tensor(path)\n","            T = video.shape[0]\n","            for i in range(T - seq_len - pred_len + 1):\n","                x = video[i:i + seq_len]\n","                y = video[i + seq_len : i + seq_len + pred_len]\n","                self.samples.append((x, y))\n","\n","    def load_video_tensor(self, path):\n","        cap = cv2.VideoCapture(path)\n","        frames = []\n","\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (160, 120))\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frame = frame.astype(np.float32) / 255.0\n","            frames.append(frame)\n","\n","        cap.release()\n","        video_np = np.stack(frames)\n","        video_np = np.transpose(video_np, (0, 3, 1, 2))\n","        return torch.from_numpy(video_np)\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        x_seq, y_seq = self.samples[idx]\n","        return x_seq, y_seq\n","\n"],"metadata":{"id":"7UlcVS_UuOih","executionInfo":{"status":"ok","timestamp":1747469175422,"user_tz":-180,"elapsed":8,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","dataset = VideoFrameDataset(video, seq_len=10)\n","dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"],"metadata":{"id":"Dr4_dS3ruSWC","executionInfo":{"status":"ok","timestamp":1747469178354,"user_tz":-180,"elapsed":2808,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML"],"metadata":{"id":"lyVKAgXsuuBd","executionInfo":{"status":"ok","timestamp":1747469178368,"user_tz":-180,"elapsed":25,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def show_sequence_from_loader(model, dataloader, device, seq_index=0):\n","    model.eval()\n","    with torch.no_grad():\n","        x_seq_batch, target_batch = next(iter(dataloader))\n","        x_seq = x_seq_batch[seq_index].unsqueeze(0).to(device)\n","        target_seq = target_batch[seq_index].cpu()\n","\n","        pred_seq = model(x_seq)[0].cpu()\n","\n","        frames = x_seq[0].cpu().permute(0, 2, 3, 1).numpy()\n","\n","        fig, ax = plt.subplots()\n","        im = ax.imshow(frames[0], cmap='gray' if frames.shape[-1] == 1 else None)\n","        ax.set_title(\"–í—Ö–æ–¥–Ω—ã–µ –∫–∞–¥—Ä—ã\")\n","        plt.axis('off')\n","\n","        def update(i):\n","            im.set_array(frames[i])\n","            ax.set_title(f\"–ö–∞–¥—Ä {i+1}\")\n","            return [im]\n","\n","        ani = animation.FuncAnimation(fig, update, frames=len(frames), interval=400, blit=True)\n","        plt.close(fig)\n","        display(HTML(ani.to_jshtml()))\n","\n","        fig, axs = plt.subplots(2, 5, figsize=(15, 5))\n","        for i in range(5):\n","            axs[0, i].imshow(pred_seq[i].permute(1, 2, 0).squeeze(), cmap='gray')\n","            axs[0, i].set_title(f\"üîÆ Pred {i+1}\")\n","            axs[0, i].axis('off')\n","\n","            axs[1, i].imshow(target_seq[i].permute(1, 2, 0).squeeze(), cmap='gray')\n","            axs\n"],"metadata":{"id":"qtKNwoSAuvYm","executionInfo":{"status":"ok","timestamp":1747469178387,"user_tz":-180,"elapsed":18,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","from tqdm import tqdm"],"metadata":{"id":"CuCsio6wyC6Q","executionInfo":{"status":"ok","timestamp":1747469178403,"user_tz":-180,"elapsed":14,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML, display\n","def visualize_fixed_sample(model, x_fixed, target_fixed, epoch):\n","    model.eval()\n","    with torch.no_grad():\n","        pred_seq = model(x_fixed)[0].cpu()\n","        input_frames = x_fixed[0].cpu().permute(0, 2, 3, 1).numpy()\n","\n","        fig, ax = plt.subplots()\n","        im = ax.imshow(input_frames[0], cmap='gray' if input_frames.shape[-1] == 1 else None)\n","        ax.set_title(\"–í—Ö–æ–¥–Ω—ã–µ –∫–∞–¥—Ä—ã\")\n","        plt.axis('off')\n","\n","        def update(i):\n","            im.set_array(input_frames[i])\n","            ax.set_title(f\"–ö–∞–¥—Ä {i+1}\")\n","            return [im]\n","\n","        ani = animation.FuncAnimation(fig, update, frames=len(input_frames), interval=400, blit=True)\n","        plt.close(fig)\n","        print(f\"\\n –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ—Å–ª–µ —ç–ø–æ—Ö–∏ {epoch}:\")\n","        display(HTML(ani.to_jshtml()))\n","\n","        fig, axs = plt.subplots(2, 5, figsize=(15, 5))\n","\n","        for i in range(5):\n","            pred_frame = pred_seq[i].permute(1, 2, 0).squeeze()\n","            true_frame = target_fixed[i].permute(1, 2, 0).squeeze()\n","\n","            axs[0, i].imshow(pred_frame, cmap='gray' if pred_frame.ndim == 2 else None)\n","            axs[0, i].set_title(f\" Pred {i+1}\")\n","            axs[0, i].axis('off')\n","\n","            axs[1, i].imshow(true_frame, cmap='gray' if true_frame.ndim == 2 else None)\n","            axs[1, i].set_title(f\" True {i+1}\")\n","            axs[1, i].axis('off')\n","\n","        plt.tight_layout()\n","        plt.show()\n"],"metadata":{"id":"O8DKC72zyzn_","executionInfo":{"status":"ok","timestamp":1747469178406,"user_tz":-180,"elapsed":1,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML, display\n","\n","\n","def train_convlstm(model, dataloader, num_epochs=10, lr=1e-3):\n","    import torch\n","    import torch.nn as nn\n","    import torch.optim as optim\n","    import matplotlib.pyplot as plt\n","    import matplotlib.animation as animation\n","    from IPython.display import HTML, display\n","    from tqdm import tqdm\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    x_fixed_batch, target_fixed_batch = next(iter(dataloader))\n","    x_fixed = x_fixed_batch[0].unsqueeze(0).to(device)\n","    target_fixed = target_fixed_batch[0].cpu()\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","\n","        for x_seq, target_seq in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n","            x_seq = x_seq.to(device)\n","            target_seq = target_seq.to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(x_seq)\n","            loss = criterion(output, target_seq)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        avg_loss = running_loss / len(dataloader)\n","        print(f\"\\n Epoch {epoch+1}/{num_epochs} ‚Äî Loss: {avg_loss:.6f}\")\n","\n","        visualize_fixed_sample(model, x_fixed, target_fixed, epoch+1)\n"],"metadata":{"id":"pDaUOT8ByE_J","executionInfo":{"status":"ok","timestamp":1747469178558,"user_tz":-180,"elapsed":144,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["num_epochs = 50\n","model = ConvLSTM(input_channels=3, hidden_channels=64)\n","train_convlstm(model, dataloader, num_epochs=10, lr=1e-3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1qsJfm5HMKVl-FM2uYiWK4Gm5ATOlCSWI"},"id":"cyg65hlcyH1G","executionInfo":{"status":"ok","timestamp":1747466510557,"user_tz":-180,"elapsed":1803994,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}},"outputId":"bd355e17-e437-46cc-9a08-412d7156100e"},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'convlstm_model.pth')"],"metadata":{"id":"kK8yqXYByKJ2","executionInfo":{"status":"ok","timestamp":1747466510693,"user_tz":-180,"elapsed":8,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["# PredRNN"],"metadata":{"id":"X7f6nRw3RZdr"}},{"cell_type":"code","source":["class ST_LSTM_Cell(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, kernel_size, bias=True):\n","        super().__init__()\n","        padding = kernel_size // 2\n","        self.conv_x = nn.Conv2d(in_channels, hidden_channels * 7, kernel_size, padding=padding, bias=bias)\n","        self.conv_h = nn.Conv2d(hidden_channels, hidden_channels * 4, kernel_size, padding=padding, bias=bias)\n","        self.conv_m = nn.Conv2d(hidden_channels, hidden_channels * 3, kernel_size, padding=padding, bias=bias)\n","        self.conv_o = nn.Conv2d(hidden_channels * 2, hidden_channels, kernel_size, padding=padding, bias=bias)\n","        self.conv_last = nn.Conv2d(hidden_channels * 2, hidden_channels, 1, padding=0, bias=bias)\n","\n","    def forward(self, x, h, c, m):\n","        x_concat = self.conv_x(x)\n","        h_concat = self.conv_h(h)\n","        m_concat = self.conv_m(m)\n","\n","        i_x, f_x, g_x, i_mx, f_mx, g_mx, o_x = torch.chunk(x_concat, 7, dim=1)\n","        i_h, f_h, g_h, o_h = torch.chunk(h_concat, 4, dim=1)\n","        i_m, f_m, g_m = torch.chunk(m_concat, 3, dim=1)\n","\n","        i = torch.sigmoid(i_x + i_h)\n","        f = torch.sigmoid(f_x + f_h)\n","        g = torch.tanh(g_x + g_h)\n","        c_new = f * c + i * g\n","\n","        i_m = torch.sigmoid(i_mx + i_m)\n","        f_m = torch.sigmoid(f_mx + f_m)\n","        g_m = torch.tanh(g_mx + g_m)\n","        m_new = f_m * m + i_m * g_m\n","\n","        mem = torch.cat([c_new, m_new], dim=1)\n","        o = torch.sigmoid(o_x + o_h + self.conv_o(mem))\n","        h_new = o * torch.tanh(self.conv_last(mem))\n","\n","        return h_new, c_new, m_new\n"],"metadata":{"id":"6AbmuVMeRY7S","executionInfo":{"status":"ok","timestamp":1747469184201,"user_tz":-180,"elapsed":7,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["class PredRNN(nn.Module):\n","    def __init__(self, input_channels=1, hidden_channels=64, kernel_size=5, num_layers=4, pred_len=5):\n","        super().__init__()\n","        self.num_layers = num_layers\n","        self.pred_len = pred_len\n","        self.hidden_channels = hidden_channels\n","\n","        self.cells = nn.ModuleList([\n","            ST_LSTM_Cell(input_channels if i == 0 else hidden_channels, hidden_channels, kernel_size)\n","            for i in range(num_layers)\n","        ])\n","\n","        self.conv_out = nn.Conv2d(hidden_channels, input_channels, kernel_size=1)\n","\n","    def forward(self, x):  # x: [B, T, C, H, W]\n","        B, T, C, H, W = x.shape\n","        h = [torch.zeros(B, self.hidden_channels, H, W, device=x.device) for _ in range(self.num_layers)]\n","        c = [torch.zeros_like(h[0]) for _ in range(self.num_layers)]\n","        m = torch.zeros_like(h[0])\n","\n","        outputs = []\n","\n","        for t in range(T + self.pred_len):\n","            x_in = x[:, t] if t < T else outputs[-1]\n","\n","            for i, cell in enumerate(self.cells):\n","                h[i], c[i], m = cell(x_in, h[i], c[i], m)\n","                x_in = h[i]\n","\n","            outputs.append(self.conv_out(x_in))\n","\n","        return torch.stack(outputs[T:], dim=1)  # [B, pred_len, C, H, W]\n"],"metadata":{"id":"NSwEeFpnRcuq","executionInfo":{"status":"ok","timestamp":1747469186025,"user_tz":-180,"elapsed":3,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["num_epochs = 50\n","model = PredRNN(input_channels=3, hidden_channels=64, pred_len=5)\n","train_convlstm(model, dataloader, num_epochs=10, lr=1e-3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":457},"id":"9cnpC8lRReVi","executionInfo":{"status":"error","timestamp":1747469244845,"user_tz":-180,"elapsed":4279,"user":{"displayName":"–°—Ç–∞–Ω–∏—Å–ª–∞–≤ –ì—É—â–∏–Ω","userId":"15072888739243516419"}},"outputId":"fff7970c-be02-4d76-b44a-3221d2317784"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10:   0%|          | 0/252 [00:04<?, ?it/s]\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 24.12 MiB is free. Process 5649 has 14.71 GiB memory in use. Of the allocated memory 14.53 GiB is allocated by PyTorch, and 70.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-9499666e16ab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_convlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-1da0cee31ad8>\u001b[0m in \u001b[0;36mtrain_convlstm\u001b[0;34m(model, dataloader, num_epochs, lr)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-60f5fba0df88>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mx_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-5f6085516830>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h, c, m)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mi_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mf_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mg_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mg_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mm_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_m\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi_m\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg_m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 24.12 MiB is free. Process 5649 has 14.71 GiB memory in use. Of the allocated memory 14.53 GiB is allocated by PyTorch, and 70.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]}]}